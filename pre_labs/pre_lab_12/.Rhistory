knitr::opts_chunk$set(echo = TRUE)
#install.packages("tidytext")
library(tidyverse)
library(tidytext)
library(janitor)
library(lubridate)
releases <- read_rds("data/cardin_releases.rds")
View(releases)
urls <- releases |> top_n(10) |>  pull(url)
release_text <- tibble(url = character(), text = character())
# loop over each url in the list of urls
for (u in urls){
# wait a fraction of a second so we don't hammer the server
Sys.sleep(0.2)
# read in the html from the url
html <- u |> read_html()
# use the xpath of the text of the release to grab it and call html_text() on it
text <- html |>
html_element(xpath="/html/body/div/div/div/div/div/div/div[2]/div[1]/div/div[4]") |>
html_text()
release_text <- release_text |> add_row(url = u, text = str_squish(text))
}
#install.packages("tidytext")
library(tidyverse)
library(tidytext)
library(janitor)
library(lubridate)
urls <- releases |> top_n(10) |>  pull(url)
release_text <- tibble(url = character(), text = character())
# loop over each url in the list of urls
for (u in urls){
# wait a fraction of a second so we don't hammer the server
Sys.sleep(0.2)
# read in the html from the url
html <- u |> read_html()
# use the xpath of the text of the release to grab it and call html_text() on it
text <- html |>
html_element(xpath="/html/body/div/div/div/div/div/div/div[2]/div[1]/div/div[4]") |>
html_text()
release_text <- release_text |> add_row(url = u, text = str_squish(text))
}
library(rvest)
install.packages("tidytext","rvest")
install.packages("tidytext", "rvest")
install.packages("tidytext")
install.packages("tidytext")
install.packages("tidytext")
library(tidyverse)
library(tidytext)
library(rvest)
library(janitor)
library(lubridate)
install.packages("tidytext")
urls <- releases |> top_n(10) |>  pull(url)
release_text <- tibble(url = character(), text = character())
# loop over each url in the list of urls
for (u in urls){
# wait a fraction of a second so we don't hammer the server
Sys.sleep(0.2)
# read in the html from the url
html <- u |> read_html()
# use the xpath of the text of the release to grab it and call html_text() on it
text <- html |>
html_element(xpath="/html/body/div/div/div/div/div/div/div[2]/div[1]/div/div[4]") |>
html_text()
release_text <- release_text |> add_row(url = u, text = str_squish(text))
}
release_text
View(releases)
releases <- releases |>
mutate(text = gsub("http.*","", text))
a_list_of_words <- c("Dog", "dog", "dog", "cat", "cat", ",")
unique(a_list_of_words)
unique_words <- releases |> select(text) |>
unnest_tokens(word, text)
View(unique_words)
unique_words |>
count(word, sort = TRUE) |>
top_n(25) |>
mutate(word = reorder(word, n)) |>
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(x = "Count",
y = "Unique words",
title = "Count of unique words found in Cardin releases")
data("stop_words")
stop_words <- stop_words |>
add_row(word = 'washington') |>
add_row(word = "ben") |>
add_row(word = "cardin") |>
add_row(word = "senator") |>
add_row(word = "senators") |>
add_row(word = "maryland") |>
add_row(word = 'federal') |>
add_row(word = 'u.s') |>
add_row(word = 'md') |>
add_row(word = 'senate') |>
add_row(word = "hollen") |>
add_row(word = "van") |>
add_row(word = "chris")
View(stop_words)
unique_words |>
anti_join(stop_words) |>
group_by(word) |>
tally(sort=TRUE) |>
mutate(percent = (n/sum(n))*100) |>
top_n(50)
View(releases)
View(unique_words)
unique_words |>
count(word, sort = TRUE) |>
top_n(25) |>
mutate(word = reorder(word, n))
unique_words |>
count(word, sort = TRUE) |>
top_n(25) |>
mutate(word = reorder(word, n)) |>
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(x = "Count",
y = "Unique words",
title = "Count of unique words found in Cardin releases")
data("stop_words")
stop_words <- stop_words |>
add_row(word = 'washington') |>
add_row(word = "ben") |>
add_row(word = "cardin") |>
add_row(word = "senator") |>
add_row(word = "senators") |>
add_row(word = "maryland") |>
add_row(word = 'federal') |>
add_row(word = 'u.s') |>
add_row(word = 'md') |>
add_row(word = 'senate') |>
add_row(word = "hollen") |>
add_row(word = "van") |>
add_row(word = "chris")
unique_words_2021 <- releases |>
filter(date < '2022-01-01') |>
select(text) |>
unnest_tokens(word, text)
unique_words_2022 <- releases |>
filter(date >= '2022-01-01') |>
select(text) |>
unnest_tokens(word, text)
unique_words_2023 <- releases |>
filter(date >= '2023-01-01') |>
select(text) |>
unnest_tokens(word, text)
unique_words_2021 |>
anti_join(stop_words) |>
group_by(word) |>
tally(sort=TRUE) |>
mutate(percent = (n/sum(n))*100) |>
top_n(10)
unique_words_2022 |>
anti_join(stop_words) |>
group_by(word) |>
tally(sort=TRUE) |>
mutate(percent = (n/sum(n))*100) |>
top_n(10)
unique_words_2023 |>
anti_join(stop_words) |>
group_by(word) |>
tally(sort=TRUE) |>
mutate(percent = (n/sum(n))*100) |>
top_n(10)
releases |>
filter(date < '2022-01-01') |>
unnest_tokens(bigram, text, token = "ngrams", n = 2) |>
separate(bigram, c("word1", "word2"), sep = " ") |>
filter(!word1 %in% stop_words$word) |>
filter(!word2 %in% stop_words$word) |>
mutate(bigram = paste(word1, word2, sep=" ")) |>
group_by(bigram) |>
tally(sort=TRUE) |>
mutate(percent = (n/sum(n))*100) |>
top_n(10)
releases |>
filter(date >= '2022-01-01', date < '2023-01-01') |>
unnest_tokens(bigram, text, token = "ngrams", n = 2) |>
separate(bigram, c("word1", "word2"), sep = " ") |>
filter(!word1 %in% stop_words$word) |>
filter(!word2 %in% stop_words$word) |>
mutate(bigram = paste(word1, word2, sep=" ")) |>
group_by(bigram) |>
tally(sort=TRUE) |>
mutate(percent = (n/sum(n))*100) |>
top_n(10)
releases |>
filter(date >= '2023-01-01') |>
unnest_tokens(bigram, text, token = "ngrams", n = 2) |>
separate(bigram, c("word1", "word2"), sep = " ") |>
filter(!word1 %in% stop_words$word) |>
filter(!word2 %in% stop_words$word) |>
mutate(bigram = paste(word1, word2, sep=" ")) |>
group_by(bigram) |>
tally(sort=TRUE) |>
mutate(percent = (n/sum(n))*100) |>
top_n(10)
bing <- get_sentiments("bing")
bing_word_counts_2021 <- unique_words_2021 |>
inner_join(bing) |>
count(word, sentiment, sort = TRUE)
bing_word_counts_2022 <- unique_words_2022 |>
inner_join(bing) |>
count(word, sentiment, sort = TRUE)
bing_word_counts_2023 <- unique_words_2023 |>
inner_join(bing) |>
count(word, sentiment, sort = TRUE)
View(bing_word_counts_2021)
View(bing_word_counts_2022)
View(bing_word_counts_2023)
View(bing)
View(bing_word_counts_2023)
